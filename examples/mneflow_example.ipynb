{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import mneflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. from MNE epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you use MNE-python, all you need is to provide your epochs file (or list of epoch files) to mneflow.produce_tfrecords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get to epochs using your mne-python pipeline\n",
    "import mne\n",
    "from mne.datasets import multimodal\n",
    "mne.set_log_level(verbose='CRITICAL')\n",
    "\n",
    "fname_raw = os.path.join(multimodal.data_path(), 'multimodal_raw.fif')\n",
    "raw = mne.io.read_raw_fif(fname_raw)\n",
    "\n",
    "cond = raw.acqparser.get_condition(raw, None)\n",
    "epochs_list = [mne.Epochs(raw, **c) for c in cond]\n",
    "\n",
    "#here we concatenate epochs because each input file contains just one condition\n",
    "#otherwise mneflow.produce_tfrecords can handle a list of epochs objects\n",
    "epochs = mne.concatenate_epochs(epochs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing epochs\n",
      "labels (940,)\n",
      "(940, 204, 151)\n",
      "Saving TFRecord# 0\n"
     ]
    }
   ],
   "source": [
    "#Specify import options\n",
    "import_opt = dict(savepath='../tfr/', #path where TFR files will be saved\n",
    "           out_name='mne_sample_epochs', #name of TFRecords files\n",
    "           input_type='epochs', #can also be \"array\"\n",
    "           picks={'meg':'grad'}, #used only if input_type is mne.epochs.Epochs or path to saved '*-epo.fif'\n",
    "           scale=True, #apply baseline_scaling?\n",
    "           crop_baseline=True,\n",
    "           decimate = 2,\n",
    "           scale_interval=(0,60), #indices in time axis corresponding to baseline interval\n",
    "           savebatch=1, # number of input files per TFRecord file           \n",
    "           save_origs=False, # whether to produce separate TFR-file for inputs in original order\n",
    "           val_size=0.1) #validations set size set to 10% of all data\n",
    "\n",
    "   \n",
    "if not os.path.exists(import_opt['savepath']):\n",
    "    os.mkdir(import_opt['savepath'])\n",
    "        \n",
    "if os.path.exists(import_opt['savepath']+'meta.pkl'):\n",
    "    meta = mneflow.load_meta(import_opt['savepath'])\n",
    "else:\n",
    "    meta = mneflow.produce_tfrecords(epochs,**import_opt)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Other data import options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Saved mne.epochs (*-epo.fif) files\n",
    "Alternatively, if your epochs are saved to disk provide a str (or list of str) with path(s) to your -epo.fif files\n",
    "\n",
    "e.g. this will work\n",
    "\n",
    "```python\n",
    "epochs.save('test_saved_epochs.fif')\n",
    "meta = mneflow.produce_tfrecords('test_saved_epochs.fif',**opt)\n",
    "```\n",
    "### 2.2. Arrays in *.mat or *.npz format\n",
    "if the first argument is str mneflow.produce_tfrecords can also accept *.mat or *.npz format\n",
    "\n",
    "e.g.\n",
    "\n",
    "```python\n",
    "data_path = '/m/nbe/scratch/braindata/izbrv/detection_data/'\n",
    "filenames = [data_path +'sub' + str(i) + '-grad.npz' for i in range(1,4)]\n",
    "meta = mneflow.produce_tfrecords(filenames,**opt)\n",
    "```\n",
    "In this case, specify iput_type='array', and also provide array_keys keyword argument\n",
    "\n",
    "e.g. \n",
    "\n",
    "```python\n",
    "array_keys={'X':'my_data_samples','y':'my_labels'}\n",
    "```\n",
    "#note that \"picks\" works only for input_type=\"epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Choose from already implemented models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lf-conv _init\n",
      "dense _init\n",
      "Initialization complete!\n"
     ]
    }
   ],
   "source": [
    "#specify optimizer parmeters\n",
    "optimizer_params = dict(l1_lambda=3e-7,\n",
    "              learn_rate=3e-4,\n",
    "              dropout = .5,\n",
    "              patience = 3,# patientce for early stopping\n",
    "              min_delta = 5e-6,\n",
    "              test_upd_batch = 20,#pseudo-real time test batch size\n",
    "              n_epochs = 1000, #total training epochs\n",
    "              eval_step = 25, #evaluate validation loss each 10 epochs\n",
    "              n_batch = 200,\n",
    "              \n",
    "              ) \n",
    "\n",
    "#specify parameters specific for the model\n",
    "#these are specific to LF-CNN]\n",
    "\n",
    "lf_params = dict(n_ls=64, #number of latent factors\n",
    "              nonlin_in = tf.identity, #input layer activation for var-cnn and lf-cnn\n",
    "              nonlin_hid = tf.nn.relu, #convolution layer activation for var-cnn and lf-cnn\n",
    "              nonlin_out = tf.identity, #output layer activation for var-cnn and lf-cnn\n",
    "              filter_length=32, #convolutional filter length for var-cnn and lf-cnn\n",
    "              pooling = 8, #convlayer pooling factor for var-cnn and lf-cnn\n",
    "              stride = 2, #stride parameter for convolution filter\n",
    "              )\n",
    "#specify the path for saving the trained model\n",
    "#here we will use the same folder as for the TFRecords\n",
    "model_path = import_opt['savepath'] \n",
    "\n",
    "#initialize the model\n",
    "model = mneflow.models.LFCNN(meta,optimizer_params,model_path,lf_params)\n",
    "\n",
    "#this will initialize the iterators over the dataset,the computational graph and the optimizer\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss 2.78795, train acc 0.0652174 val loss 2.78331, val acc 0.106383\n",
      "epoch 25, train_loss 0.429146, train acc 0.869565 val loss 1.17445, val acc 0.553191\n",
      "epoch 50, train_loss 0.0394581, train acc 1 val loss 0.595269, val acc 0.712766\n",
      "epoch 75, train_loss 0.0107133, train acc 1 val loss 0.510771, val acc 0.851064\n",
      "epoch 100, train_loss 0.00499535, train acc 1 val loss 0.341246, val acc 0.882979\n",
      "* Patience count 1\n",
      "epoch 150, train_loss 0.003583, train acc 1 val loss 0.305725, val acc 0.893617\n",
      "* Patience count 2\n",
      "* Patience count 3\n",
      "early stopping...\n",
      "WARNING:tensorflow:From /u/62/zubarei1/unix/.conda/envs/py3ml/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../tfr/lf-cnn-mne_sample_epochs\n",
      "stopped at: epoch 200, val loss 0.305725, val acc 0.893617\n",
      "Trained in 146.07s\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "start = time()\n",
    "model.train()\n",
    "stop = time() - start\n",
    "print('Trained in {:.2f}s'.format(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compute_patterns(output='patterns')\n",
    "#explore output layer weights\n",
    "#TODO: Fix bug related to varying sampling rates and pooling factors\n",
    "#f = model.plot_out_weihts()\n",
    "\n",
    "#explore informative spatial patterns(LF-CNN only)\n",
    "#TODO: Fix visualizations\n",
    "f = model.plot_patterns(sensor_layout='Vectorview-grad', sorting='best', spectra=True, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Or specify your own neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete!\n",
      "epoch 0, train_loss 1.75461, train acc 0.630435 val loss 1.8381, val acc 0.531915\n",
      "epoch 50, train_loss 1.27403, train acc 1 val loss 1.48677, val acc 0.829787\n",
      "epoch 100, train_loss 1.27403, train acc 1 val loss 1.47608, val acc 0.808511\n",
      "* Patience count 1\n",
      "* Patience count 2\n",
      "epoch 250, train_loss 1.27402, train acc 1 val loss 1.46621, val acc 0.819149\n",
      "* Patience count 3\n",
      "early stopping...\n",
      "INFO:tensorflow:Restoring parameters from ../tfr/my_own-mne_sample_epochs\n",
      "stopped at: epoch 300, val loss 1.46621, val acc 0.819149\n",
      "Trained in 118.53s\n"
     ]
    }
   ],
   "source": [
    "#let's make a simple linear classifier using all channels*timepoints as features with keras\n",
    "params = dict(l1_lambda=0,\n",
    "              learn_rate=3e-4,\n",
    "              dropout = .5,\n",
    "              patience = 3,# patientce for early stopping\n",
    "              min_delta = 5e-3, #note the increased convergence threshold1\n",
    "              test_upd_batch = 20,#pseudo-real time test batch size\n",
    "              n_epochs = 1000, #total training epochs\n",
    "              #nonlin_out=tf.identity,\n",
    "              eval_step = 50, #evaluate validation loss each 10 epochs\n",
    "              n_batch = 200) #training batch size) \n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "\n",
    "class MyNetwork(mneflow.models.Model):\n",
    "    #all you need to do is to override the computational graph with your own\n",
    "    def _build_graph(self):\n",
    "        self.h_params['architecture'] = 'my_own'\n",
    "        input_main   = self.X\n",
    "        flatten      = Flatten()(input_main)\n",
    "        dense        = Dense(self.h_params['n_classes'], kernel_constraint = max_norm(0.5))(flatten)\n",
    "        y_pred      = Activation('softmax')(dense)\n",
    "        return y_pred\n",
    "    \n",
    "m2 = MyNetwork(meta,params,model_path)\n",
    "m2.build()\n",
    "start = time()\n",
    "m2.train()\n",
    "stop = time() - start\n",
    "print('Trained in {:.2f}s'.format(stop))\n",
    "\n",
    "\n",
    "# #evaluate performance\n",
    "# test_accs = m2.evaluate_performance(meta['orig_paths'], batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: across-subject/leave-one-subject-out example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
