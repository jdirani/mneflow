{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying your own neural network with MNEflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the same dataset as in the basic mneflow example\n",
    "import os\n",
    "from time import time\n",
    "os.chdir('/m/nbe/work/zubarei1/mneflow/')\n",
    "\n",
    "import mne\n",
    "from mne.datasets import multimodal\n",
    "import mneflow\n",
    "\n",
    "mne.set_log_level(verbose='CRITICAL')\n",
    "\n",
    "fname_raw = os.path.join(multimodal.data_path(), 'multimodal_raw.fif')\n",
    "raw = mne.io.read_raw_fif(fname_raw)\n",
    "\n",
    "cond = raw.acqparser.get_condition(raw, None)\n",
    "condition_names = [k for c in cond for k,v in c['event_id'].items()]\n",
    "\n",
    "epochs_list = [mne.Epochs(raw, **c) for c in cond]\n",
    "epochs = mne.concatenate_epochs(epochs_list)\n",
    "#pick only planar gradiometers\n",
    "epochs = epochs.pick_types(meg='grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify import options\n",
    "import_opt = dict(savepath='../tfr/', #path where TFR files will be saved\n",
    "                   out_name='mne_sample_epochs', #name of TFRecords files\n",
    "                   scale=True, #apply baseline_scaling\n",
    "                   crop_baseline=True, #remove baseline interval after scaling\n",
    "                   decimate = 2,\n",
    "                   scale_interval=(0,60), #indices in time axis corresponding to baseline interval\n",
    "                   val_size=0.15, #validations set size set to 15% of all data\n",
    "                   overwrite=False) \n",
    "\n",
    "##since meta file already exists and overwrite=False produce_tfrecords does not need to repeat the preprocessing\n",
    "meta = mneflow.produce_tfrecords(epochs,**import_opt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make a simple peceptron-like classifier using all channels*timepoints as features with keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "class MyNetwork(mneflow.models.Model):\n",
    "    #In the simplest case all you need to do is to override the computational graph with your own\n",
    "    def build_graph(self):\n",
    "        self.scope = 'custom_model'\n",
    "        input_main   = self.X\n",
    "        flatten      = Flatten()(input_main)\n",
    "        dense        = Dense(self.n_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "        y_pred      = Activation('softmax')(dense)\n",
    "        return y_pred\n",
    "    \n",
    "    #the same can be done with Optimizer._set_optimizer method if you need to use a custom optimization pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /m/nbe/work/zubarei1/mneflow/mneflow/data.py:107: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:tensorflow:From /u/62/zubarei1/unix/.conda/envs/py3ml/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "y_pred: (?, 8)\n",
      "Initialization complete!\n",
      "i 0, tr_loss 2.10412, tr_acc 0.1 v_loss 2.06491, v_acc 0.141844\n",
      "i 100, tr_loss 1.2745, tr_acc 1 v_loss 1.49248, v_acc 0.843972\n",
      "i 200, tr_loss 1.2743, tr_acc 1 v_loss 1.48332, v_acc 0.851064\n",
      "i 300, tr_loss 1.27422, tr_acc 1 v_loss 1.47863, v_acc 0.858156\n",
      "i 400, tr_loss 1.27417, tr_acc 1 v_loss 1.4754, v_acc 0.865248\n",
      "i 500, tr_loss 1.27414, tr_acc 1 v_loss 1.47262, v_acc 0.865248\n",
      "i 600, tr_loss 1.27412, tr_acc 1 v_loss 1.47017, v_acc 0.858156\n",
      "i 700, tr_loss 1.27411, tr_acc 1 v_loss 1.46785, v_acc 0.858156\n",
      "i 800, tr_loss 1.27409, tr_acc 1 v_loss 1.46579, v_acc 0.865248\n",
      "i 900, tr_loss 1.27408, tr_acc 1 v_loss 1.46396, v_acc 0.879433\n",
      "i 1000, tr_loss 1.27408, tr_acc 1 v_loss 1.46243, v_acc 0.879433\n",
      "* Patience count 1\n",
      "i 1100, tr_loss 1.27408, tr_acc 1 v_loss 1.46493, v_acc 0.879433\n",
      "* Patience count 2\n",
      "i 1200, tr_loss 1.27407, tr_acc 1 v_loss 1.4626, v_acc 0.879433\n",
      "i 1300, tr_loss 1.27406, tr_acc 1 v_loss 1.46076, v_acc 0.879433\n",
      "i 1400, tr_loss 1.27406, tr_acc 1 v_loss 1.45921, v_acc 0.886525\n",
      "i 1500, tr_loss 1.27406, tr_acc 1 v_loss 1.45784, v_acc 0.886525\n",
      "i 1600, tr_loss 1.27405, tr_acc 1 v_loss 1.45661, v_acc 0.886525\n",
      "i 1700, tr_loss 1.27405, tr_acc 1 v_loss 1.4555, v_acc 0.886525\n",
      "i 1800, tr_loss 1.27405, tr_acc 1 v_loss 1.4545, v_acc 0.886525\n",
      "i 1900, tr_loss 1.27404, tr_acc 1 v_loss 1.4536, v_acc 0.886525\n",
      "i 2000, tr_loss 1.27404, tr_acc 1 v_loss 1.45277, v_acc 0.886525\n",
      "i 2100, tr_loss 1.27404, tr_acc 1 v_loss 1.452, v_acc 0.886525\n",
      "i 2200, tr_loss 1.27404, tr_acc 1 v_loss 1.45128, v_acc 0.879433\n",
      "i 2300, tr_loss 1.27404, tr_acc 1 v_loss 1.45059, v_acc 0.879433\n",
      "i 2400, tr_loss 1.27404, tr_acc 1 v_loss 1.44994, v_acc 0.879433\n",
      "i 2500, tr_loss 1.27404, tr_acc 1 v_loss 1.44932, v_acc 0.879433\n",
      "i 2600, tr_loss 1.27404, tr_acc 1 v_loss 1.44873, v_acc 0.879433\n",
      "i 2700, tr_loss 1.27403, tr_acc 1 v_loss 1.44817, v_acc 0.879433\n",
      "i 2800, tr_loss 1.27403, tr_acc 1 v_loss 1.44763, v_acc 0.879433\n",
      "i 2900, tr_loss 1.27403, tr_acc 1 v_loss 1.44712, v_acc 0.87234\n",
      "i 3000, tr_loss 1.27403, tr_acc 1 v_loss 1.44663, v_acc 0.87234\n"
     ]
    }
   ],
   "source": [
    "optimizer_params = dict(l1_lambda=0, \n",
    "                        l2_lambda=0, \n",
    "                        learn_rate=3e-4, \n",
    "                        task= 'classification')\n",
    "\n",
    "graph_specs = dict(model_path=import_opt['savepath'],\n",
    "                   dropout=1.)\n",
    "\n",
    "dataset = mneflow.Dataset(meta, train_batch = 200, class_subset=None,\n",
    "                                 pick_channels=None,decim=None)\n",
    "\n",
    "optimizer = mneflow.Optimizer(**optimizer_params)\n",
    "\n",
    "model = MyNetwork(dataset, optimizer, graph_specs)\n",
    "\n",
    "model.build()\n",
    "model.train(n_iter=3000,eval_step=100,min_delta=1e-6,early_stopping=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since our custom model inherits some methods from the parent class we can e.g. plot the confusion matrix easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model.plot_cm(dataset='validation', class_names = condition_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations of LFCNN and VARCNN can be implemented easily using the mneflow.layers module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's say we want to stack two layers of VARCNN\n",
    "import tensorflow as tf\n",
    "from mneflow.layers import DeMixing, VARConv, Dense\n",
    "class VARCNN2(mneflow.models.Model):\n",
    "    def build_graph(self):\n",
    "        self.scope = 'var2'\n",
    "        self.demix = DeMixing(n_ls=self.specs['n_ls'])\n",
    "\n",
    "        self.tconv1 = VARConv(scope=\"conv\", n_ls=self.specs['n_ls'],\n",
    "                              nonlin_out=tf.nn.relu,\n",
    "                              filter_length=self.specs['filter_length'],\n",
    "                              stride=self.specs['stride'],\n",
    "                              pooling=self.specs['pooling'],\n",
    "                              padding=self.specs['padding'])\n",
    "        self.tconv2 = VARConv(scope=\"conv\", n_ls=self.specs['n_ls']//2,\n",
    "                              nonlin_out=tf.nn.relu,\n",
    "                              filter_length=self.specs['filter_length']//4,\n",
    "                              stride=self.specs['stride'],\n",
    "                              pooling=self.specs['pooling']//4,\n",
    "                              padding=self.specs['padding'])\n",
    "\n",
    "        self.fin_fc = Dense(size=self.n_classes,\n",
    "                            nonlin=tf.identity, dropout=self.rate)\n",
    "\n",
    "        y_pred = self.fin_fc(self.tconv2(self.tconv1(self.demix(self.X))))\n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-mix init : OK\n",
      "conv init : OK\n",
      "conv init : OK\n",
      "fc init : OK\n",
      "y_pred: (?, 8)\n",
      "Initialization complete!\n",
      "i 0, tr_loss 3.85249, tr_acc 0.14 v_loss 2.96505, v_acc 0.134752\n",
      "i 500, tr_loss 0.137925, tr_acc 0.975 v_loss 0.906255, v_acc 0.695035\n",
      "i 1000, tr_loss 0.0252244, tr_acc 0.995 v_loss 0.700253, v_acc 0.780142\n",
      "i 1500, tr_loss 0.0281842, tr_acc 0.995 v_loss 0.645575, v_acc 0.801418\n",
      "i 2000, tr_loss 0.00755402, tr_acc 1 v_loss 0.630235, v_acc 0.815603\n",
      "i 2500, tr_loss 0.00786491, tr_acc 1 v_loss 0.600168, v_acc 0.836879\n",
      "i 3000, tr_loss 0.00739726, tr_acc 1 v_loss 0.546477, v_acc 0.843972\n",
      "i 3500, tr_loss 0.00670859, tr_acc 1 v_loss 0.539367, v_acc 0.87234\n",
      "* Patience count 1\n",
      "i 4000, tr_loss 0.00663283, tr_acc 1 v_loss 0.55158, v_acc 0.851064\n",
      "* Patience count 2\n",
      "i 4500, tr_loss 0.00998582, tr_acc 1 v_loss 0.563259, v_acc 0.879433\n",
      "* Patience count 3\n",
      "early stopping...\n",
      "WARNING:tensorflow:From /u/62/zubarei1/unix/.conda/envs/py3ml/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../tfr/var2-mne_sample_epochs\n",
      "stopped at: epoch 5000, val loss 0.539367, val acc 0.87234\n"
     ]
    }
   ],
   "source": [
    "#similarly, we specify the optimizer and parameters specific to the computational graph\n",
    "optimizer_params = dict(l1_lambda=3e-6, learn_rate=3e-4)\n",
    "\n",
    "graph_specs = dict(n_ls=32,  # number of latent factors\n",
    "                   filter_length=17,  # convolutional filter length\n",
    "                   pooling=4,  # convlayer pooling factor\n",
    "                   stride=2,  # stride parameter for convolution filter\n",
    "                   padding='SAME',\n",
    "                   model_path=import_opt['savepath'],\n",
    "                   dropout=.5)\n",
    "\n",
    "dataset = mneflow.Dataset(meta, train_batch = 200, class_subset=None,\n",
    "                                 pick_channels=None,decim=None)\n",
    "\n",
    "optimizer = mneflow.Optimizer(**optimizer_params)\n",
    "\n",
    "var2 = VARCNN2(dataset, optimizer, graph_specs)\n",
    "\n",
    "var2.build()\n",
    "#This model is more complex and will reuqire much more iterations (and time) to train\n",
    "var2.train(n_iter=10000,eval_step=500,min_delta=1e-6,early_stopping=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, increase in model capacity/complexity did not result in the improvement in performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
